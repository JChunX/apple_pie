_target_: skrl.agents.torch.ppo.PPO
memory: 
  _target_: skrl.memories.torch.RandomMemory
  memory_size: 512
cfg:
  rollouts: 512               # number of rollouts before updating
  learning_epochs: 8           # number of learning epochs during each update
  mini_batches: 2              # number of mini batches during each learning epoch
  discount_factor: 0.99        # discount factor (gamma)
  lambda: 0.95                 # TD(lambda) coefficient (lam) for computing returns and advantages

  learning_rate: 1e-3                  # learning rate
  learning_rate_scheduler: null        # learning rate scheduler class (see torch.optim.lr_scheduler)
  learning_rate_scheduler_kwargs: {}   # learning rate scheduler's kwargs (e.g. {"step_size": 1e-3})

  state_preprocessor: null             # state preprocessor class (see skrl.resources.preprocessors)
  state_preprocessor_kwargs: {}        # state preprocessor's kwargs (e.g. {"size": env.observation_space})
  value_preprocessor: null             # value preprocessor class (see skrl.resources.preprocessors)
  value_preprocessor_kwargs: {}        # value preprocessor's kwargs (e.g. {"size": 1})

  random_timesteps: 0          # random exploration steps
  learning_starts: 0           # learning starts after this many steps

  grad_norm_clip: 0.5              # clipping coefficient for the norm of the gradients
  ratio_clip: 0.2                  # clipping coefficient for computing the clipped surrogate objective
  value_clip: 0.2                  # clipping coefficient for computing the value loss (if clip_predicted_values is True)
  clip_predicted_values: False     # clip predicted values during value loss computation

  entropy_loss_scale: 0.0      # entropy loss scaling factor
  value_loss_scale: 1.0        # value loss scaling factor

  kl_threshold: 0              # KL divergence threshold for early stopping
  rewards_shaper: null         # rewards shaping function: Callable(reward, timestep, timesteps) -> reward
  time_limit_bootstrap: False  # bootstrap at timeout termination (episode truncation)

  mixed_precision: False       # enable automatic mixed precision for higher performance
  experiment:
    wandb: False
    wandb_kwargs: null